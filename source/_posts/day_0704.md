---
title: "Python 기초문법"
date: '2022-07-04'
---


## 확률적 경사 하강법
- 점진적 학습 (step, 보폭)
- 학습률
- XGBoost, LightGBM, 딥러닝(이미지 분류, 자연어 처리, 옵티마이저)

### 
- 신경망 이미지 데이터, 자연어
- 자율주행 하루 데이터 1TB --> 학습
- 한꺼번에 다 모델을 학습 어려움
  + 샘플링, 배치, 에포크, 오차(=손실=loss)가 가장 작은 지점을 찾아야 함. 
- 결론적으로, 확률적 경사 하강법

### 손실함수
- 로지스틱 손실 함수



```python
import pandas as pd 
fish = pd.read_csv("https://bit.ly/fish_csv_data")
fish.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159 entries, 0 to 158
    Data columns (total 6 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Species   159 non-null    object 
     1   Weight    159 non-null    float64
     2   Length    159 non-null    float64
     3   Diagonal  159 non-null    float64
     4   Height    159 non-null    float64
     5   Width     159 non-null    float64
    dtypes: float64(5), object(1)
    memory usage: 7.6+ KB
    

- 입력 데이터와 타깃 데이터 분리


```python
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

fish_input.shape, fish_target.shape
```




    ((159, 5), (159,))



- 훈련 세트와 테스트 데이터 분리


```python
from sklearn.model_selection import train_test_split 
train_input, test_input, train_target, test_target = train_test_split(
    # input, target, 옵션... 
    fish_input, fish_target, random_state=42
)
```

- 훈련 세트와 테스트 세트의 특성 표준화
  + 무게, 길이, 대각선 길이, 높이, 너비 
- 표준화 처리 진행
  


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# train_scaled[:5]
```

## 모델링
- 확률적 경사 하강법 


```python
from sklearn.linear_model import SGDClassifier 
sc = SGDClassifier(loss = 'log', max_iter = 10, random_state=42)

sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    

- partial_fit() 메서드 사용하면 추가 학습. 


```python
sc.partial_fit(train_scaled, train_target) 
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.8151260504201681
    0.85
    

## 에포크와 과대/과소적합
- 에포크 숫자가 적으면 --> 덜 학습
- early_stopping
  + 에포크 숫자를 1000, 손실 10, 9, 8, , 3
  + 3에 도달한 시점이 150 


```python
import numpy as np 
sc = SGDClassifier(loss='log', random_state = 42)
train_score = []
test_score = []

classes = np.unique(train_target)

# 300번 에포크 훈련을 반복
# 훈련 할 때마다, train_score, test_score 추가를 한다. 
for _ in range(0, 300):
  sc.partial_fit(train_scaled, train_target, classes = classes)
  train_score.append(sc.score(train_scaled, train_target))
  test_score.append(sc.score(test_scaled, test_target)) 

```

- 시각화 


```python
import matplotlib.pyplot as plt 
plt.plot(train_score)
plt.plot(test_score)
plt.legend(["train", "test"])
plt.show()
```


    
![png](/images/python_0704/output_17_0.png)
    


## 결정트리
- wine 데이터 가져오기


```python
import pandas as pd 
wine = pd.read_csv('https://bit.ly/wine_csv_data')
print(wine.head())
```

       alcohol  sugar    pH  class
    0      9.4    1.9  3.51    0.0
    1      9.8    2.6  3.20    0.0
    2      9.8    2.3  3.26    0.0
    3      9.8    1.9  3.16    0.0
    4      9.4    1.9  3.51    0.0
    

- 데이터 가공하기 


```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```


```python
wine['class'].value_counts()
```




    1.0    4898
    0.0    1599
    Name: class, dtype: int64



- 훈련데이터 분리


```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)

train_input.shape, test_input.shape, train_target.shape, test_target.shape
```




    ((5197, 3), (1300, 3), (5197,), (1300,))



- 표준화 처리


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

- 모델 만들기


```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt 
from sklearn.tree import plot_tree 

# criterion{“gini”, “entropy”, “log_loss”}, default=”gini”
dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 8, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```

    0.89532422551472
    0.8569230769230769
    


    
![png](/images/python_0704/output_28_1.png)
    



```python
# criterion{“gini”, “entropy”, “log_loss”}, default=”gini”
dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 1, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(10, 7))
plot_tree(dt, max_depth = 4, 
          filled=True, 
          feature_names = ['alcohol', 'sugar', 'pH'])
plt.show()
```

    0.7579372715027901
    0.7376923076923076
    


    
![png](/images/python_0704/output_29_1.png)
    


- 훈련 정확도는 99.6%
- 테스트 정확도는 85.9% 
--> 과대적합이 일어남

### 노드란 무엇인가? 
- 0이면 레드 와인 : 
- 1이면 화이트 와인 : 4898


```python
plt.figure(figsize=(10, 7))
plot_tree(dt, 
          max_depth = 1, 
          filled=True, 
          feature_names = ['alcohol', 'sugar', 'pH'])
plt.show()
```


    
![png](/images/python_0704/output_32_0.png)
    


- 불순도
  + 한 범주 안에서 서로 다른 데이터가 얼마나 섞여 있는 나타냄
  + 흰색과 검은색이 각각 50개 섞여 있다.
    + 불순도 최대 - 0.5 
  + 흰색과 검은색이 완전 100% 분리가 됨
    + 흰색 노드 불순도 최소 - 0
    + 검은색 노드 불순도 최소 - 0
- 엔드로피(Entropy)
  + 불확실한 정도를 의미함. 0 ~ 1 사이
  + 흰색과 검은색이 각각 50개 섞여 있다.
    + 엔트로피 최대 - 1
  + 흰색과 검은색이 완전 100% 분리가 됨
    + 흰색 노드 엔트로피 최소 - 0
    + 검은색 노드 엔트로피 최소 - 0

### 특성 중요도
- 어떤 특성이 결정 트리 모델에 영향을 주었는가? 


```python
print(dt.feature_importances_)
```

    [0. 1. 0.]
    

## 현업에서의 적용
- 현업에서 DecisionTreeClassifier (1970년대)
- 랜덤포레스트, XGBooost 하이퍼파라미터 매우 많음



## 검증 세트 
- 훈련세트와 테스트세트
- 훈련 : 교과서 공부하는 것 훈련세트, 모의평가
- 검증 : 강남대성 모의고사 문제지 
- 테스트 : 6월 / 9월 
- 실전 : 수능




```python
import pandas as pd 
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')
# print(wine.head())

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# 훈련 80%
# 테스트 20% 
train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size = 0.2, random_state=42
)

train_input.shape, test_input.shape, train_target.shape, test_target.shape

```




    ((5197, 3), (1300, 3), (5197,), (1300,))




```python
# 훈련 80%
# 검증 20%
sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size = 0.2, random_state=42
)

sub_input.shape, val_input.shape, sub_target.shape, val_target.shape
```




    ((4157, 3), (1040, 3), (4157,), (1040,))



- 훈련데이터 : sub_input, sub_target
- 검증데이터 : val_input, val_target
- 테스트데이터 : test_input, test_target

- 모형 만들기


```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print("훈련 성과 : ", dt.score(sub_input, sub_target))
print("검증 성과 : ", dt.score(val_input, val_target))
print("마지막 최종 : ", dt.score(test_input, test_target))
```

    훈련 성과 :  0.9971133028626413
    검증 성과 :  0.864423076923077
    마지막 최종 :  0.8569230769230769
    

- 훈련 : 87% 
- 검증 : 86% 
-----

- 최종 : 55% 

### 교차 검증
- 데이터 셋을 반복 분할
- For loop
- 샘플링 편향적일 수 있음
- 교차검증을 한다고 해서, 정확도가 무조건 올라간다!? (X) 
- 모형을 안정적으로 만들어 준다
  + 과대적합 방지 


```python
import numpy as np 
from sklearn.model_selection import KFold

df = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# 데이터를 K 폴드로 나눈다. 
folds = KFold(n_splits=5, shuffle = True)
for train_idx, valid_idx in folds.split(df):
  print(f'훈련 데이터 : {df[train_idx]}, 검증 데이터 : {df[valid_idx]}')
```

    훈련 데이터 : [ 1  2  3  4  5  7  9 10], 검증 데이터 : [6 8]
    훈련 데이터 : [ 1  2  3  5  6  8  9 10], 검증 데이터 : [4 7]
    훈련 데이터 : [ 1  4  5  6  7  8  9 10], 검증 데이터 : [2 3]
    훈련 데이터 : [1 2 3 4 5 6 7 8], 검증 데이터 : [ 9 10]
    훈련 데이터 : [ 2  3  4  6  7  8  9 10], 검증 데이터 : [1 5]
    

- 교차 검증 함수


```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
print("평균 : ", np.mean(scores['test_score']))
```

    {'fit_time': array([0.00848246, 0.00733709, 0.00798798, 0.00785208, 0.00716829]), 'score_time': array([0.00093985, 0.00078893, 0.00088668, 0.0008297 , 0.00073981]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균 :  0.855300214703487
    

- StratifiedKFold 사용


```python
from sklearn.model_selection import StratifiedKFold 
scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())
print(scores)
print("평균 : ", np.mean(scores['test_score']))
```

    {'fit_time': array([0.00848508, 0.00715542, 0.00766325, 0.00738764, 0.00697041]), 'score_time': array([0.00083804, 0.00075769, 0.00085258, 0.00069857, 0.00066876]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균 :  0.855300214703487
    

- 10 폴드 교차검증을 수행


```python
from sklearn.model_selection import StratifiedKFold 
splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv = splitter)
print(scores)
print("평균 : ", np.mean(scores['test_score']))
```

    {'fit_time': array([0.01321006, 0.00938487, 0.00905538, 0.0082438 , 0.00809932,
           0.01311088, 0.00815201, 0.00798512, 0.00820947, 0.00827479]), 'score_time': array([0.00104856, 0.00078392, 0.0007627 , 0.00069618, 0.00068617,
           0.00076795, 0.00069237, 0.00065351, 0.00070119, 0.00103307]), 'test_score': array([0.83461538, 0.87884615, 0.85384615, 0.85384615, 0.84615385,
           0.87307692, 0.85961538, 0.85549133, 0.85163776, 0.86705202])}
    평균 :  0.8574181117533719
    

## 하이퍼파라미터 튜닝
- 그리드 서치
  + 사람이 수동적으로 입력
  + max_depth : [1, 3, 7]
- 랜덤 서치
  + 사람이 범위만 지정
  + max_depth : 1 ~ 10 / by random

- 베이지안 옵티마이제이션
- 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술을 AutoML이라고 함. 
  + 예) PyCaret
- 각 모델마다 적게는 1-2개에서, 많게는 5-6개의 매개변수를 제공한다. 
  + XGBoost 100개...!? 
- 하이퍼파라미터와 동시에 교차검증을 수행.
  + 미친짓!? 

교차검증 5번
교차검증 1번 돌 때, Max Depth 3번 적용 
- 총 결괏값 3 x 5 x 2 나옴

- Max Depth = 1, 3, 7
- Criterion = gini, entropy



```python
from sklearn.model_selection import GridSearchCV
params = {
    'criterion' : ['gini', 'entropy'],
    'max_depth' : [1, 3, 7],
    'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]
}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs = -1)
gs.fit(train_input, train_target)
```




    GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
                 param_grid={'criterion': ['gini', 'entropy'],
                             'max_depth': [1, 3, 7],
                             'min_impurity_decrease': [0.0001, 0.0002, 0.0003,
                                                       0.0004, 0.0005]})




```python
print("best :" , gs.best_estimator_)
dt = gs.best_estimator_
```

    best : DecisionTreeClassifier(max_depth=7, min_impurity_decrease=0.0005,
                           random_state=42)
    
